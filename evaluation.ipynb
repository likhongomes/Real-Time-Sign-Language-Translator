{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WeG-lVhYx9"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#setup: import libraries\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (8, 5)"
      ],
      "id": "B0WeG-lVhYx9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEiOTMSnhYx9"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#each experiment corresponds to one trained model + its evaluation JSON\n",
        "\n",
        "#Configure experiment result files here. Each entry should point to an evaluation_results.json file created by utils.evaluate_model\n",
        "\n",
        "#example: uncomment and adjust paths when files exist\n",
        "# EXPERIMENTS = [\n",
        "#     {\n",
        "#         'name': 'transformer_baseline',\n",
        "#         'results_path': 'evaluation_transformer/evaluation_results.json',\n",
        "#     },\n",
        "#     {\n",
        "#         'name': 'lstm_baseline',\n",
        "#         'results_path': 'evaluation_lstm/evaluation_results.json',\n",
        "#     },\n",
        "# ]\n",
        "\n",
        "#FILL THIS IN ONCE EVALUATION HAS BEEN RUN\n",
        "EXPERIMENTS = []\n",
        "EXPERIMENTS"
      ],
      "id": "WEiOTMSnhYx9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FnlNfbZhYx-"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#helper function to load eval results\n",
        "\n",
        "def load_results(path: Path):\n",
        "    \"\"\"Load a single evaluation_results.json file if it exists.\n",
        "    Returns a dict or None if the file is missing.\n",
        "    \"\"\"\n",
        "    if not path.exists():\n",
        "        print(f\"[WARN] Results file not found: {path}\")\n",
        "        return None\n",
        "    with path.open('r') as f:\n",
        "        return json.load(f)"
      ],
      "id": "7FnlNfbZhYx-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8S3HQEEhYx_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#building a small table summarizing each models:\n",
        "# top1 accuracy\n",
        "# top5 accuracy\n",
        "# macro f1score\n",
        "# weighted f1 score\n",
        "# num of eval samples\n",
        "\n",
        "summary_rows = []\n",
        "\n",
        "for exp in EXPERIMENTS:\n",
        "    name = exp.get('name', 'unnamed')\n",
        "    results_path = Path(exp.get('results_path', ''))\n",
        "    res = load_results(results_path)\n",
        "    if res is None:\n",
        "        continue\n",
        "\n",
        "    report = res.get('classification_report', {})\n",
        "    macro = report.get('macro avg', {})\n",
        "    weighted = report.get('weighted avg', {})\n",
        "\n",
        "    summary_rows.append({\n",
        "        'model': name,\n",
        "        'accuracy': res.get('accuracy', None),\n",
        "        'top5_accuracy': res.get('top5_accuracy', None),\n",
        "        'macro_f1': macro.get('f1-score', None),\n",
        "        'weighted_f1': weighted.get('f1-score', None),\n",
        "        'num_samples': res.get('num_samples', None),\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "if summary_df.empty:\n",
        "    print(\"No evaluation results loaded yet.\\n\"\n",
        "          \"Once you have run utils.evaluate_model(...) for at least one model,\\n\"\n",
        "          \"add the corresponding JSON path to EXPERIMENTS above and re-run.\")\n",
        "else:\n",
        "    display(summary_df)"
      ],
      "id": "V8S3HQEEhYx_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgqqxkhBhYyA"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#simple bar plot visualization comparing accuracy and f1 scores across models\n",
        "if 'summary_df' in globals() and not summary_df.empty:\n",
        "    metrics_to_plot = ['accuracy', 'macro_f1', 'weighted_f1']\n",
        "    available_metrics = [m for m in metrics_to_plot if m in summary_df.columns]\n",
        "\n",
        "    if available_metrics:\n",
        "        ax = summary_df.plot(\n",
        "            x='model',\n",
        "            y=available_metrics,\n",
        "            kind='bar'\n",
        "        )\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylim(0, 1)\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Model performance comparison')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No numeric metrics available to plot.\")\n",
        "else:\n",
        "    print(\"No summary table available yet; run the previous cell after loading results.\")"
      ],
      "id": "KgqqxkhBhYyA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNFYckJKhYyA"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#per class metrics for a selected model for error analysis\n",
        "\n",
        "#select which experiment to inspect in detail such as, 'transformer_baseline'\n",
        "SELECTED_EXPERIMENT_NAME = None\n",
        "\n",
        "selected_exp = None\n",
        "for exp in EXPERIMENTS:\n",
        "    if exp.get('name') == SELECTED_EXPERIMENT_NAME:\n",
        "        selected_exp = exp\n",
        "        break\n",
        "\n",
        "if selected_exp is None:\n",
        "    print(\"No SELECTED_EXPERIMENT_NAME set or experiment not found.\\n\"\n",
        "          \"Set SELECTED_EXPERIMENT_NAME to one of the names in EXPERIMENTS.\")\n",
        "else:\n",
        "    res = load_results(Path(selected_exp['results_path']))\n",
        "    if res is not None:\n",
        "        report = res.get('classification_report', {})\n",
        "        rows = []\n",
        "        for label, stats in report.items():\n",
        "            #skip global entries\n",
        "            if label in ['accuracy', 'macro avg', 'weighted avg']:\n",
        "                continue\n",
        "            if not isinstance(stats, dict):\n",
        "                continue\n",
        "\n",
        "            rows.append({\n",
        "                'label': label,\n",
        "                'precision': stats.get('precision', None),\n",
        "                'recall': stats.get('recall', None),\n",
        "                'f1': stats.get('f1-score', None),\n",
        "                'support': stats.get('support', None),\n",
        "            })\n",
        "\n",
        "        per_class_df = pd.DataFrame(rows)\n",
        "        if per_class_df.empty:\n",
        "            print(\"No per-class statistics found in classification report.\")\n",
        "        else:\n",
        "            #sort by F1 descending for convenience\n",
        "            per_class_df = per_class_df.sort_values('f1', ascending=False)\n",
        "            display(per_class_df.head(10))\n",
        "            print(\"\\nLowest 10 F1-score classes:\")\n",
        "            display(per_class_df.tail(10))\n",
        "    else:\n",
        "        print(\"Could not load results for selected experiment.\")"
      ],
      "id": "SNFYckJKhYyA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_l_VfUKhYyB"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#optional: opens saved confusion matrix images\n",
        "for exp in EXPERIMENTS:\n",
        "    name = exp.get('name', 'unnamed')\n",
        "    results_dir = Path(exp.get('results_path', '')).parent\n",
        "    cm_path = results_dir / 'confusion_matrix.png'\n",
        "    if cm_path.exists():\n",
        "        print(f\"Found confusion matrix for {name}: {cm_path}\")\n",
        "    else:\n",
        "        print(f\"No confusion matrix found for {name} (looked for {cm_path}).\")"
      ],
      "id": "s_l_VfUKhYyB"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}